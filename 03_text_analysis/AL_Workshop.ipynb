{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df7b8b9",
   "metadata": {},
   "source": [
    "# SICSS 2025: Active Learning Workshop\n",
    "\n",
    "> Diletta Goglia, Uppsala University InfoLab | <diletta.goglia@it.uu.se> | BlueSky: @dilettagoglia.bsky.social\n",
    "\n",
    "Welcome to the AL workshop, as part of the Computational Text Analaysis day! In this notebook, you will learn how to automatically label a dataset, starting from a small set of human-labeled texts. In particular, you will:\n",
    "\n",
    "* train and evaluate a RoBERTa model for multi-label text annotation \n",
    "* fine-tune the model\n",
    "* predict (generate) annotations for unlabelled texts\n",
    "\n",
    "## Corpus description\n",
    "\n",
    "We will use the data collected by Manika Lamba and Hendrik Erz in their paper \"[Thanking the World](https://www.sciencedirect.com/science/article/pii/S2543925124000287)\". They collected ~1200 acknowledgment sections from theses for a total fo ~20k individual sentences. They manually annotated a random sample of 900 sentences, according to the type of support they contain (academic, moral, finantial, technical, religious, library, access to data, or other), and trained a RoBERTa-base transformer model to annotate the remaining data (yes, AL!). In this notebook, we will go through all the necessary steps required for this final phase.\n",
    "\n",
    "If you need help, please ask during the workshop or contact me via email :) have fun with AL!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54287e26",
   "metadata": {},
   "source": [
    "# The Preliminaries: \n",
    "\n",
    "## Defining the Concept\n",
    "\n",
    "The first step of the work consisted in manually annotating the training dataset according to the **support labels**. It is not part of the workshop, but we will use:\n",
    "\n",
    "* the support labels: to train the classifier to assign them\n",
    "* the file with human annotations (*AL_gold_data.tsv*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01ef57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPP_LABELS = [\n",
    "\"Academic\",\n",
    "\"Moral\",\n",
    "\"Tech\",\n",
    "\"Data\",\n",
    "\"Library\",\n",
    "\"Finance\",\n",
    "\"Religious\",\n",
    "\"Unknown\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76314b1f",
   "metadata": {},
   "source": [
    "## Installing the necessary packages\n",
    "\n",
    "Make sure to install packages according to how you have set up Python. If you use plain `pip`, here is how you can install them:\n",
    "\n",
    "```bash\n",
    "python -m pip install tqdm          # Used for progress bars\n",
    "python -m pip install transformers  # To use the models\n",
    "python -m pip install torch         # PyTorch for model handling\n",
    "python -m pip install evaluate      # For validation metrics\n",
    "python -m pip install matplotlib    # For plotting\n",
    "python -m pip install seaborn       # Again, for plotting\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01512c5",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526a3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm \n",
    "from transformers import RobertaTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer \n",
    "import torch \n",
    "from torch import nn\n",
    "import evaluate\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed289439",
   "metadata": {},
   "source": [
    "## Parameters setting\n",
    "\n",
    "This phase is the initial setup step where you define key configuration values that control how your model will behave. This phase does not involve training or loading data yet! It’s just about defining your environment and behavior.\n",
    "\n",
    "Many ML operations (e.g., shuffling and sampling) involve randomness. Setting a random seed ensures that your code produces the same results every time you run it, which is essential for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff540dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducible results\n",
    "seed = 1989\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.mps.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48dc51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When running on CPU, this somehow makes sure training times do not degrade\n",
    "# see: https://discuss.pytorch.org/t/training-time-gets-slower-and-slower-on-cpu/145483/3\n",
    "torch.set_flush_denormal(True)\n",
    "\n",
    "# Select the device\n",
    "# If you have a MacBook (with a Silicon chip), you have \"mps\" available. On\n",
    "# Windows or Linux, if you have an nVidia GPU, you have CUDA available.\n",
    "# Otherwise, use the CPU.\n",
    "\n",
    "device = torch.device(\"cpu\") # Fallback: CPU\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "  device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8e7a9f",
   "metadata": {},
   "source": [
    "# Annotated Data Loading\n",
    "\n",
    "Now we load the human-annotated dataset and we create both the training and the validation datasets for our model. Then we put the data into a format that our model can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3596268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences: list[str] = list()\n",
    "labels: list[int] = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c930e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_samples():\n",
    "  \"\"\"Reads in the gold data and yields tuples (sentence, labels)\"\"\"\n",
    "  with open(\"AL_gold_data.tsv\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    next(fp) # Skip header\n",
    "    for line in fp:\n",
    "      cols = line.strip().split(\"\\t\")\n",
    "      sentence = cols[0]\n",
    "      label = np.argmax(np.asarray([int(x) for x in cols[1:9]]))\n",
    "      yield (sentence, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cc42fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence, label in read_samples():\n",
    "    sentences.append(sentence)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c06cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a random train/valid split\n",
    "rand = np.random.default_rng()\n",
    "train_idx: list[int] = rand.choice(len(sentences), size=round(len(sentences) * 0.8), replace = False)\n",
    "valid_idx = set(range(len(sentences))).difference(set(train_idx))\n",
    "print(f\"Datasets prepared! We are training with {len(train_idx)} training and {len(valid_idx)} validation samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a41c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class helps us organize inputs and labels into a format that PyTorch models understand.\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "  \"\"\"Basically copied verbatim from https://huggingface.co/transformers/v3.5.1/custom_datasets.html\"\"\"\n",
    "  def __init__(self, encodings, labels):\n",
    "    self.encodings = encodings\n",
    "    self.labels = labels\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "    item['labels'] = self.labels[idx]\n",
    "    return item\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cf5254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This will download the roberta-base tokenizer model to your device.\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ecaaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts a list of raw text sentences and their labels into a PyTorch dataset\n",
    "\n",
    "def sentences_to_data_loader (sentences: list[str], labels: list[int]):\n",
    "  \"\"\"Takes a list of sentences, a batch size and a list of integer labels and constructs a dataset from that.\"\"\"\n",
    "  tok = tokenizer(sentences, padding=\"max_length\", truncation=True, return_tensors='pt', return_attention_mask=True)\n",
    "  return CustomDataset(tok, torch.tensor(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a46637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create two datasets with this information:\n",
    "support_train = sentences_to_data_loader([sentences[i] for i in train_idx], labels=[labels[i] for i in train_idx])\n",
    "support_valid = sentences_to_data_loader([sentences[i] for i in valid_idx], labels=[labels[i] for i in valid_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dad942",
   "metadata": {},
   "source": [
    "# Model Training Evaluation\n",
    "\n",
    "We set up the metrics used to validate the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8f7cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to determine the best model (ideally f1, otherwise loss works)\n",
    "metric = 'f1'\n",
    "is_greater_better = True\n",
    "\n",
    "f1_metric = evaluate.load('f1')\n",
    "acc_metric = evaluate.load('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3111927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_support(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = torch.from_numpy(predictions)\n",
    "    predictions = nn.functional.softmax(predictions, dim=-1)\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    # Calculates one F1 per label, so we should have an array with 8 elements\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=None)['f1']\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)['accuracy']\n",
    "\n",
    "    # NOTE: We define the F1 here as the average score of all categories\n",
    "    avg_f1 = np.mean(f1)\n",
    "\n",
    "    return { 'f1': avg_f1, 'accuracy': acc }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc8d919",
   "metadata": {},
   "source": [
    "# Finetuning\n",
    "\n",
    "We can finally train the RoBERTa model ! The following block sets up all the training parameters, downloads a pretrained language model (RoBERTa), wraps everything into a Trainer class, and then trains the model on the labeled dataset. It saves the best version automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8c8e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"model\",\n",
    "    eval_strategy = \"epoch\", # Print results after each epoch\n",
    "    save_strategy = \"epoch\", # If loading best model, save + eval need to match\n",
    "    per_device_train_batch_size=8, # Default is 8\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=15.0, # default 3\n",
    "    learning_rate = 5e-05, # default: 5e-05\n",
    "    adam_epsilon = 1e-8, # Taken from Rubing's script\n",
    "    load_best_model_at_end = True, # Default: False\n",
    "    metric_for_best_model = metric,\n",
    "    greater_is_better = is_greater_better,\n",
    "    # use_mps_device=True #  <-- UNCOMMENT this line if you are using a MacOS machine\n",
    "  )\n",
    "\n",
    "# NOTE: This will download the RoBERTa Base model to your machine\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=len(SUPP_LABELS), # How many labels should the model learn to assign?\n",
    "    problem_type=\"single_label_classification\"\n",
    "  )\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=args,\n",
    "  train_dataset=support_train,\n",
    "  eval_dataset=support_valid,\n",
    "  compute_metrics=compute_metrics_support\n",
    ")\n",
    "\n",
    "print(\"Training support category model!\")\n",
    "trainer.train()\n",
    "trainer.save_model(\"finetuned_model\")\n",
    "print(\"Model trained!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a0f89e",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "\n",
    "We are ready to annotate the rest of the texts. We first load the fine-tuned model from the folder where we saved it after training.\n",
    "This restores all the learned weights so we can use the model for making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44556f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"finetuned_model\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98406d02",
   "metadata": {},
   "source": [
    "## Annotating\n",
    "\n",
    "We load the corpus that we want to annotate, and we predicts labels for each sentence using our fine-tuned model. We write the results to a file (*AL_predictions.tsv*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b7806",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(\"AL_corpus.tsv\", sep=\"\\t\")\n",
    "print(f\"Corpus size: {len(corpus)} sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26566045",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ce177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"AL_predictions.tsv\", \"w\") as fp:\n",
    "    fp.write(f\"year\\tsentence\\tsupport_label\\n\")\n",
    "    for row in tqdm(corpus.itertuples(), total=len(corpus), desc=\"Predicting\", dynamic_ncols=True):\n",
    "        tok = tokenizer(row.sentence, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "        tok = tok.to(device)\n",
    "\n",
    "        output = model(**tok)\n",
    "        predictions = output.logits.detach().squeeze(0).cpu().numpy()\n",
    "        supp_label = np.argmax(predictions)\n",
    "\n",
    "        fp.write(f\"{row.year}\\t{row.sentence}\\t{SUPP_LABELS[supp_label]}\\n\")\n",
    "        fp.flush() # Make sure we can watch as the file fills\n",
    "\n",
    "    print(\"Prediction done! You can find the predictions in the ''AL_predictions.tsv'' file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d469956",
   "metadata": {},
   "source": [
    "## Visualizing the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e438fedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read AL_predictions.tsv as pandas DataFrame\n",
    "predictions_df = pd.read_csv(\"AL_predictions.tsv\", sep=\"\\t\", encoding=\"utf-8\")\n",
    "print(predictions_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f141215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now plot the distribution of support labels\n",
    "predictions_df['support_label'].value_counts().plot(kind='bar')\n",
    "plt.title(\"Distribution of support labels\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6db2b81",
   "metadata": {},
   "source": [
    "**What can you observe?** Are some categories much more common than others? Does anything look surprising?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c495773",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = predictions_df.groupby([\"year\", \"support_label\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "# Line plot of support label frequency over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=counts, x=\"year\", y=\"count\", hue=\"support_label\", marker=\"o\", palette=\"Set2\")\n",
    "\n",
    "plt.title(\"Support label frequency over time\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of sentences\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.xticks(sorted(predictions_df[\"year\"].unique()))\n",
    "plt.tight_layout()\n",
    "plt.legend(title=\"Label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feae961",
   "metadata": {},
   "source": [
    "What can you observe from this temporal perspective?\n",
    "Do certain categories appear more in earlier or later years? Are there any noticeable shifts, spikes, or disappearances over time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bfae8e",
   "metadata": {},
   "source": [
    "## Extra: Handling authentication with the Hugging Face Hub \n",
    "\n",
    "Hugging Face is a company and open-source community that provides tools, models, and libraries for working with machine learning—especially NLP and LLMs.\n",
    "\n",
    "To use Hugging Face’s models (especially large ones or those requiring authentication), you need an access token. This token links your Hugging Face account to your code securely.\n",
    "\n",
    "**How to Create a Hugging Face Access Token:**\n",
    "\n",
    "* Create a [Hugging Face account](https://huggingface.co) (if you don't already have one).\n",
    "* After logging in, go to your Access Tokens page and click on \"new token\".\n",
    "* Choose a name (e.g., sicss-token), select the role, and click \"create\".\n",
    "* Copy the token and past it in the cell below.\n",
    "* **Never share your token publicly!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1302e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "my_token = \"PAST YOUR TOKEN HERE\"\n",
    "login(token=my_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d731573",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Thank you for following along with this workshop!\n",
    "\n",
    "If you have further questions regarding do not hesitate to contact me: <diletta.goglia@it.uu.se> | @dilettagoglia on social media.\n",
    "\n",
    "Special thanks to Hendrik for his help and inspiration for this notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
