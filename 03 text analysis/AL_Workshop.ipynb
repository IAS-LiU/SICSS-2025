{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df7b8b9",
   "metadata": {},
   "source": [
    "# SICSS 2025: Active Learning Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54287e26",
   "metadata": {},
   "source": [
    "# The Preliminaries: Defining the Concept\n",
    "\n",
    "Also done previously but not part of the lab: Manually labelling a training dataset (the gold data file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c01ef57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPP_LABELS = [\n",
    "\"Academic\",\n",
    "\"Moral\",\n",
    "\"Tech\",\n",
    "\"Data\",\n",
    "\"Library\",\n",
    "\"Finance\",\n",
    "\"Religious\",\n",
    "\"Unknown\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01512c5",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "526a3a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/nlpbert/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm # progress bars\n",
    "from transformers import RobertaTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer # Huggingface models and utilities\n",
    "import torch # PyTorch for model handling\n",
    "from torch import nn\n",
    "import evaluate\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0084d8c9",
   "metadata": {},
   "source": [
    "## Handling authentication with the Hugging Face Hub \n",
    "\n",
    "To use Hugging Faceâ€™s models (especially large ones or those requiring authentication), you need an access token. This token links your Hugging Face account to your code securely.\n",
    "\n",
    "ðŸ”‘ How to Create a Hugging Face Access Token:\n",
    "\n",
    "* Create a [Hugging Face account](https://huggingface.co) (if you don't already have one).\n",
    "* After logging in, go to your Access Tokens page and click on \"new token\".\n",
    "* Choose a name (e.g., sicss-token), select the role, and click \"create\".\n",
    "* Copy the token and past it in the cell below.\n",
    "* **Never share your token publicly!**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a9f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "my_token = \"PAST YOUR TOKEN HERE\"\n",
    "login(token=my_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed289439",
   "metadata": {},
   "source": [
    "## Train a RoBERTa model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dff540dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducible results\n",
    "seed = 1989\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.mps.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e48dc51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When running on CPU, this somehow makes sure training times do not degrade\n",
    "# see: https://discuss.pytorch.org/t/training-time-gets-slower-and-slower-on-cpu/145483/3\n",
    "torch.set_flush_denormal(True)\n",
    "\n",
    "# Select the device\n",
    "# If you have a MacBook (with a Silicon chip), you have \"mps\" available. On\n",
    "# Windows or Linux, if you have an nVidia GPU, you have CUDA available.\n",
    "# Otherwise, use the CPU.\n",
    "\n",
    "device = torch.device(\"cpu\") # Fallback: CPU\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "  device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8e7a9f",
   "metadata": {},
   "source": [
    "# Annotated Data Loading\n",
    "\n",
    "Load the annotated gold standard data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3596268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences: list[str] = list()\n",
    "labels: list[int] = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c930e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_samples ():\n",
    "  \"\"\"Reads in the gold data and yields tuples (sentence, labels)\"\"\"\n",
    "  with open(\"AL_gold_data.tsv\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    next(fp) # Skip header\n",
    "    for line in fp:\n",
    "      cols = line.strip().split(\"\\t\")\n",
    "      sentence = cols[0]\n",
    "      label = np.argmax(np.asarray([int(x) for x in cols[1:9]]))\n",
    "      yield (sentence, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97cc42fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence, label in read_samples():\n",
    "    sentences.append(sentence)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c06cd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets prepared! We are training with 722 training and 181 validation samples.\n"
     ]
    }
   ],
   "source": [
    "# We create a random train/valid split\n",
    "rand = np.random.default_rng()\n",
    "train_idx: list[int] = rand.choice(len(sentences), size=round(len(sentences) * 0.8), replace = False)\n",
    "valid_idx = set(range(len(sentences))).difference(set(train_idx))\n",
    "print(f\"Datasets prepared! We are training with {len(train_idx)} training and {len(valid_idx)} validation samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9a41c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "  \"\"\"Basically copied verbatim from https://huggingface.co/transformers/v3.5.1/custom_datasets.html\"\"\"\n",
    "  def __init__(self, encodings, labels):\n",
    "    self.encodings = encodings\n",
    "    self.labels = labels\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "    item['labels'] = self.labels[idx]\n",
    "    return item\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25cf5254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This will download the roberta-base tokenizer model to your device.\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7ecaaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_data_loader (sentences: list[str], labels: list[int]):\n",
    "  \"\"\"Takes a list of sentences, a batch size and a list of integer labels and constructs a dataset from that.\"\"\"\n",
    "  tok = tokenizer(sentences, padding=\"max_length\", truncation=True, return_tensors='pt', return_attention_mask=True)\n",
    "  return CustomDataset(tok, torch.tensor(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2a46637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create two datasets with this information:\n",
    "support_train = sentences_to_data_loader([sentences[i] for i in train_idx], labels=[labels[i] for i in train_idx])\n",
    "support_valid = sentences_to_data_loader([sentences[i] for i in valid_idx], labels=[labels[i] for i in valid_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dad942",
   "metadata": {},
   "source": [
    "# Model Training Evaluation\n",
    "\n",
    "Set up evaluation and the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce8f7cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to determine the best model (ideally f1, otherwise loss works)\n",
    "metric = 'f1'\n",
    "is_greater_better = True\n",
    "\n",
    "f1_metric = evaluate.load('f1')\n",
    "acc_metric = evaluate.load('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3111927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_support(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = torch.from_numpy(predictions)\n",
    "    predictions = nn.functional.softmax(predictions, dim=-1)\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    # Calculates one F1 per label, so we should have an array with ... nine (?) elements\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=None)['f1']\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)['accuracy']\n",
    "\n",
    "    # NOTE: We define the F1 here as the average score of all categories\n",
    "    avg_f1 = np.mean(f1)\n",
    "\n",
    "    return { 'f1': avg_f1, 'accuracy': acc }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc8d919",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c8c8e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training support category model!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1365' max='1365' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1365/1365 23:50, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.129135</td>\n",
       "      <td>0.258045</td>\n",
       "      <td>0.640884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.114910</td>\n",
       "      <td>0.255467</td>\n",
       "      <td>0.635359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.310800</td>\n",
       "      <td>0.409537</td>\n",
       "      <td>0.674033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.204556</td>\n",
       "      <td>0.565022</td>\n",
       "      <td>0.701657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.345684</td>\n",
       "      <td>0.521445</td>\n",
       "      <td>0.696133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.733200</td>\n",
       "      <td>1.503726</td>\n",
       "      <td>0.551395</td>\n",
       "      <td>0.696133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.733200</td>\n",
       "      <td>1.870560</td>\n",
       "      <td>0.433100</td>\n",
       "      <td>0.674033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.733200</td>\n",
       "      <td>1.931399</td>\n",
       "      <td>0.478047</td>\n",
       "      <td>0.651934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.733200</td>\n",
       "      <td>1.829501</td>\n",
       "      <td>0.581153</td>\n",
       "      <td>0.723757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.733200</td>\n",
       "      <td>1.934010</td>\n",
       "      <td>0.552193</td>\n",
       "      <td>0.712707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.123500</td>\n",
       "      <td>1.965436</td>\n",
       "      <td>0.557969</td>\n",
       "      <td>0.701657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.123500</td>\n",
       "      <td>1.936381</td>\n",
       "      <td>0.548134</td>\n",
       "      <td>0.707182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.123500</td>\n",
       "      <td>2.061476</td>\n",
       "      <td>0.523860</td>\n",
       "      <td>0.685083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.123500</td>\n",
       "      <td>2.038600</td>\n",
       "      <td>0.543504</td>\n",
       "      <td>0.701657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.123500</td>\n",
       "      <td>2.028609</td>\n",
       "      <td>0.564673</td>\n",
       "      <td>0.707182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EVALUATION|SUPPORT] F1(avg): 0.2580449967878826 | F1(individual): 0.6756756756756757, 0.6896551724137931, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6990291262135923 | Accuracy: 0.6408839779005525\n",
      "\n",
      "\n",
      "[EVALUATION|SUPPORT] F1(avg): 0.2554673721340388 | F1(individual): 0.6857142857142857, 0.7160493827160493, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6419753086419753 | Accuracy: 0.6353591160220995\n",
      "\n",
      "\n",
      "[EVALUATION|SUPPORT] F1(avg): 0.4095365009343504 | F1(individual): 0.717948717948718, 0.7160493827160493, 0.0, 0.0, 0.4444444444444444, 0.0, 0.6666666666666666, 0.7311827956989247 | Accuracy: 0.6740331491712708\n",
      "\n",
      "\n",
      "[EVALUATION|SUPPORT] F1(avg): 0.565022425954832 | F1(individual): 0.75, 0.7476635514018691, 0.18181818181818182, 0.13333333333333333, 0.5, 0.75, 0.6666666666666666, 0.7906976744186046 | Accuracy: 0.7016574585635359\n",
      "\n",
      "\n",
      "[EVALUATION|SUPPORT] F1(avg): 0.5214448417462016 | F1(individual): 0.7659574468085106, 0.6966292134831461, 0.47058823529411764, 0.0, 0.4, 0.4444444444444444, 0.6666666666666666, 0.7272727272727273 | Accuracy: 0.6961325966850829\n",
      "\n",
      "\n",
      "[EVALUATION|SUPPORT] F1(avg): 0.5513950030849565 | F1(individual): 0.7633587786259542, 0.7216494845360825, 0.4444444444444444, 0.0, 0.3333333333333333, 0.75, 0.6666666666666666, 0.7317073170731707 | Accuracy: 0.6961325966850829\n",
      "\n",
      "\n",
      "[EVALUATION|SUPPORT] F1(avg): 0.4331002804396734 | F1(individual): 0.7755102040816326, 0.6976744186046512, 0.42857142857142855, 0.0, 0.0, 0.2222222222222222, 0.6666666666666666, 0.6741573033707865 | Accuracy: 0.6740331491712708\n",
      "\n",
      "\n",
      "[EVALUATION|SUPPORT] F1(avg): 0.4780470521541951 | F1(individual): 0.7346938775510204, 0.6888888888888889, 0.25, 0.2222222222222222, 0.2857142857142857, 0.3333333333333333, 0.6666666666666666, 0.6428571428571429 | Accuracy: 0.6519337016574586\n",
      "\n",
      "\n",
      "[EVALUATION|SUPPORT] F1(avg): 0.5811528163937802 | F1(individual): 0.8, 0.7472527472527473, 0.42857142857142855, 0.2222222222222222, 0.2857142857142857, 0.8, 0.6666666666666666, 0.6987951807228916 | Accuracy: 0.7237569060773481\n",
      "\n",
      "\n",
      "[EVALUATION|SUPPORT] F1(avg): 0.5521928349171643 | F1(individual): 0.7727272727272727, 0.7608695652173914, 0.5333333333333333, 0.0, 0.18181818181818182, 0.8, 0.6666666666666666, 0.7021276595744681 | Accuracy: 0.712707182320442\n",
      "\n",
      "\n",
      "[EVALUATION|SUPPORT] F1(avg): 0.5579687378224611 | F1(individual): 0.7801418439716312, 0.7252747252747253, 0.625, 0.0, 0.2, 0.8, 0.6666666666666666, 0.6666666666666666 | Accuracy: 0.7016574585635359\n",
      "\n",
      "\n",
      "[EVALUATION|SUPPORT] F1(avg): 0.5481335237457001 | F1(individual): 0.782608695652174, 0.7252747252747253, 0.625, 0.0, 0.2222222222222222, 0.6666666666666666, 0.6666666666666666, 0.6966292134831461 | Accuracy: 0.7071823204419889\n",
      "\n",
      "\n",
      "[EVALUATION|SUPPORT] F1(avg): 0.5238599764082198 | F1(individual): 0.7659574468085106, 0.7252747252747253, 0.5333333333333333, 0.0, 0.18181818181818182, 0.6666666666666666, 0.6666666666666666, 0.6511627906976745 | Accuracy: 0.6850828729281768\n",
      "\n",
      "\n",
      "[EVALUATION|SUPPORT] F1(avg): 0.5435043670098018 | F1(individual): 0.782608695652174, 0.7252747252747253, 0.625, 0.0, 0.2, 0.6666666666666666, 0.6666666666666666, 0.6818181818181818 | Accuracy: 0.7016574585635359\n",
      "\n",
      "\n",
      "[EVALUATION|SUPPORT] F1(avg): 0.564672971493227 | F1(individual): 0.7883211678832117, 0.7252747252747253, 0.625, 0.0, 0.36363636363636365, 0.6666666666666666, 0.6666666666666666, 0.6818181818181818 | Accuracy: 0.7071823204419889\n",
      "\n"
     ]
    },
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: IoError(Os { code: 28, kind: StorageFull, message: \"No space left on device\" })",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSafetensorError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining support category model!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m trainer.train()\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfinetuned_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel trained!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlpbert/lib/python3.12/site-packages/transformers/trainer.py:3911\u001b[39m, in \u001b[36mTrainer.save_model\u001b[39m\u001b[34m(self, output_dir, _internal_call)\u001b[39m\n\u001b[32m   3908\u001b[39m         \u001b[38;5;28mself\u001b[39m.model_wrapped.save_checkpoint(output_dir)\n\u001b[32m   3910\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3911\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3913\u001b[39m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[32m   3914\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.push_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlpbert/lib/python3.12/site-packages/transformers/trainer.py:4015\u001b[39m, in \u001b[36mTrainer._save\u001b[39m\u001b[34m(self, output_dir, state_dict)\u001b[39m\n\u001b[32m   4013\u001b[39m             torch.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n\u001b[32m   4014\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4015\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4016\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_safetensors\u001b[49m\n\u001b[32m   4017\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.processing_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4020\u001b[39m     \u001b[38;5;28mself\u001b[39m.processing_class.save_pretrained(output_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlpbert/lib/python3.12/site-packages/transformers/modeling_utils.py:3717\u001b[39m, in \u001b[36mPreTrainedModel.save_pretrained\u001b[39m\u001b[34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[39m\n\u001b[32m   3712\u001b[39m     gc.collect()\n\u001b[32m   3714\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[32m   3715\u001b[39m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[32m   3716\u001b[39m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3717\u001b[39m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mformat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3718\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3719\u001b[39m     save_function(shard, os.path.join(save_directory, shard_file))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlpbert/lib/python3.12/site-packages/safetensors/torch.py:286\u001b[39m, in \u001b[36msave_file\u001b[39m\u001b[34m(tensors, filename, metadata)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_file\u001b[39m(\n\u001b[32m    256\u001b[39m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    257\u001b[39m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    258\u001b[39m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    259\u001b[39m ):\n\u001b[32m    260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[33;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[32m    262\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[43mserialize_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mSafetensorError\u001b[39m: Error while serializing: IoError(Os { code: 28, kind: StorageFull, message: \"No space left on device\" })"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"model\",\n",
    "    eval_strategy = \"epoch\", # Print results after each epoch\n",
    "    save_strategy = \"epoch\", # If loading best model, save + eval need to match\n",
    "    per_device_train_batch_size=8, # Default is 8\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=15.0, # default 3\n",
    "    learning_rate = 5e-05, # default: 5e-05\n",
    "    adam_epsilon = 1e-8, # Taken from Rubing's script\n",
    "    load_best_model_at_end = True, # Default: False\n",
    "    metric_for_best_model = metric,\n",
    "    greater_is_better = is_greater_better,\n",
    "    # use_mps_device=True #  <-- UNCOMMENT this line if you are using a MacOS machine\n",
    "  )\n",
    "\n",
    "# NOTE: This will download the RoBERTa Base model to your machine\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=len(SUPP_LABELS), # How many labels should the model learn to assign?\n",
    "    problem_type=\"single_label_classification\"\n",
    "  )\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=args,\n",
    "  train_dataset=support_train,\n",
    "  eval_dataset=support_valid,\n",
    "  compute_metrics=compute_metrics_support\n",
    ")\n",
    "\n",
    "print(\"Training support category model!\")\n",
    "trainer.train()\n",
    "trainer.save_model(\"finetuned_model\")\n",
    "print(\"Model trained!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a0f89e",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44556f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"finetuned_model\")\n",
    "model.to(device)\n",
    "\n",
    "# predicted = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98406d02",
   "metadata": {},
   "source": [
    "# Annotating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e87b7806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 20170 sentences.\n"
     ]
    }
   ],
   "source": [
    "corpus = pd.read_csv(\"AL_corpus.tsv\", sep=\"\\t\")\n",
    "print(f\"Corpus size: {len(corpus)} sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26566045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1927</td>\n",
       "      <td>Thanks are due to Dr* W. S. Pord, Assistant Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1927</td>\n",
       "      <td>Mr.V.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1927</td>\n",
       "      <td>C. Kersey, Assistant Superintendent of Schools...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1927</td>\n",
       "      <td>School principals have been very kind to allow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1927</td>\n",
       "      <td>Records of attendance, enrollment, etc were wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20165</th>\n",
       "      <td>2020</td>\n",
       "      <td>I also acknowledge my students and colleagues ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20166</th>\n",
       "      <td>2020</td>\n",
       "      <td>I offer a special acknowledgment to my dear fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20167</th>\n",
       "      <td>2020</td>\n",
       "      <td>The friendship of so many people has helped ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20168</th>\n",
       "      <td>2020</td>\n",
       "      <td>responded to the survey, interest in service e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20169</th>\n",
       "      <td>2020</td>\n",
       "      <td>Significant contributing factors cited by resp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20170 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year                                           sentence\n",
       "0      1927  Thanks are due to Dr* W. S. Pord, Assistant Pr...\n",
       "1      1927                                              Mr.V.\n",
       "2      1927  C. Kersey, Assistant Superintendent of Schools...\n",
       "3      1927  School principals have been very kind to allow...\n",
       "4      1927  Records of attendance, enrollment, etc were wi...\n",
       "...     ...                                                ...\n",
       "20165  2020  I also acknowledge my students and colleagues ...\n",
       "20166  2020  I offer a special acknowledgment to my dear fr...\n",
       "20167  2020  The friendship of so many people has helped ma...\n",
       "20168  2020  responded to the survey, interest in service e...\n",
       "20169  2020  Significant contributing factors cited by resp...\n",
       "\n",
       "[20170 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4ce177b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   6%|â–Œ         | 1229/20170 [00:46<11:50, 26.66it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m tok = tok.to(device)\n\u001b[32m      7\u001b[39m output = model(**tok)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m predictions = \u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.numpy()\n\u001b[32m      9\u001b[39m supp_label = np.argmax(predictions)\n\u001b[32m     11\u001b[39m fp.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow.year\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mrow.sentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mSUPP_LABELS[supp_label]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "with open(\"AL_predictions.tsv\", \"w\") as fp:\n",
    "    fp.write(f\"filename\\tsentence\\tsupport_label\\n\")\n",
    "    for row in tqdm(corpus.itertuples(), total=len(corpus), desc=\"Predicting\", dynamic_ncols=True):\n",
    "        tok = tokenizer(row.sentence, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "        tok = tok.to(device)\n",
    "\n",
    "        output = model(**tok)\n",
    "        predictions = output.logits.detach().squeeze(0).cpu().numpy()\n",
    "        supp_label = np.argmax(predictions)\n",
    "\n",
    "        fp.write(f\"{row.year}\\t{row.sentence}\\t{SUPP_LABELS[supp_label]}\\n\")\n",
    "        fp.flush() # Make sure we can watch as the file fills\n",
    "\n",
    "    print(\"Prediction done! You can find the predictions in the ''AL_predictions.tsv'' file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1302e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
